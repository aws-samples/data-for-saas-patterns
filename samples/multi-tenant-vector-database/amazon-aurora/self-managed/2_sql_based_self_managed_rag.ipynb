{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1 - SQL Query Based Self Managed RAG with Aurora Vector Database\n",
    "\n",
    "Prerequisites before you run these scripts : \n",
    "1. Deploy an Aurora PostgreSQL Cluster with RDS Data API enabled\n",
    "2. Create the vector db schema, table & index using self-managed/1_build_vector_db_on_aurora.sql\n",
    "3. Note the cluster ARN from the Aurora PostgreSQL Cluster\n",
    "4. Note the secret Key ARN for the Aurora cluster database username/password.\n",
    "5. Create a secret key for the database user app_user (used for RLS)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the boto3 library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U boto3==1.34.84\n",
    "%pip install pypdf\n",
    "%pip install langchain==0.2.7\n",
    "%pip install langchain-community==0.2.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and clients bedrock_agent, bedrock-agent-runtime, bedrock-runtime,S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import uuid\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "region_name = \"us-west-2\"\n",
    "\n",
    "# Create the client for Bedrock\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=region_name)\n",
    "\n",
    "# Create the client for RDS Data API\n",
    "rdsData = boto3.client(service_name=\"rds-data\", region_name=region_name)\n",
    "\n",
    "# Aurora Database Configurations\n",
    "db_name = \"postgres\"\n",
    "cluster_arn = \"<update aurora cluster arn>\"\n",
    "\n",
    "# User postgres\n",
    "secret_arn = (\n",
    "    \"<update postgres secret ARN>\"\n",
    ")\n",
    "# User - app_user\n",
    "secret_arn_rls = (\n",
    "    \"<update app_user secret ARN>\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_vector_embeddings(data):\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"inputText\": data,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Invoke model\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body,\n",
    "        modelId=\"amazon.titan-embed-text-v1\",\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\",\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to insert vector embeddings into vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_vector_db(embedding, chunk, metadata, tenantid):\n",
    "    # Insert query parameters\n",
    "    param1 = {\"name\": \"id\", \"value\": {\"stringValue\": str(uuid.uuid4())}}\n",
    "    param2 = {\"name\": \"embedding\", \"value\": {\"stringValue\": str(embedding)}}\n",
    "    param3 = {\"name\": \"chunks\", \"value\": {\"stringValue\": chunk}}\n",
    "    param4 = {\"name\": \"metadata\", \"value\": {\"stringValue\": json.dumps(metadata)}, \"typeHint\": \"JSON\"}\n",
    "    param5 = {\"name\": \"tenantid\", \"value\": {\"stringValue\": tenantid}}\n",
    "    paramSet = [param1, param2, param3, param4, param5]\n",
    "\n",
    "    # Invoke the Insert query using RDS Data API\n",
    "    response = rdsData.execute_statement(\n",
    "        resourceArn=cluster_arn,\n",
    "        secretArn=secret_arn,\n",
    "        database=db_name,\n",
    "        sql=\"INSERT INTO self_managed.kb(id, embedding, chunks, metadata, tenantid) VALUES (:id::uuid,:embedding::vector,:chunks, :metadata, :tenantid::varchar(10))\",\n",
    "        parameters=paramSet,\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to query the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vector_database(embedding):\n",
    "    paramSet = [{\"name\": \"embedding\", \"value\": {\"stringValue\": str(embedding)}}]\n",
    "\n",
    "    response = rdsData.execute_statement(\n",
    "        resourceArn=cluster_arn,\n",
    "        secretArn=secret_arn,\n",
    "        database=db_name,\n",
    "        sql=\"SELECT id,metadata,chunks FROM self_managed.kb ORDER BY embedding <=> :embedding::vector LIMIT 5; \",\n",
    "        parameters=paramSet,\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Function to query the vector database using RLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to query the vector database using L2 distance\n",
    "def query_vector_database_using_rls(embedding, tenantid):\n",
    "    paramSet = [{\"name\": \"embedding\", \"value\": {\"stringValue\": str(embedding)}}]\n",
    "\n",
    "    query = \"SET self_managed.kb.tenantid =\\\"\"+ str(tenantid) +\"\\\"\"\n",
    "    print(query)\n",
    "\n",
    "    tr = rdsData.begin_transaction(\n",
    "        resourceArn = cluster_arn,\n",
    "        secretArn = secret_arn_rls,\n",
    "        database = db_name)\n",
    "\n",
    "    rdsData.execute_statement(resourceArn=cluster_arn,\n",
    "                            secretArn=secret_arn_rls,\n",
    "                            database=db_name,\n",
    "                            sql=query,\n",
    "                            transactionId = tr['transactionId'])\n",
    "\n",
    "    response = rdsData.execute_statement(resourceArn=cluster_arn,\n",
    "                                        secretArn=secret_arn_rls,\n",
    "                                        database=db_name,\n",
    "                                        sql='SELECT id,tenantid,metadata,chunks FROM self_managed.kb ORDER BY embedding <=> :embedding::vector LIMIT 5; ',\n",
    "                                        parameters=paramSet,\n",
    "                                        transactionId = tr['transactionId'])\n",
    "\n",
    "    cr = rdsData.commit_transaction(\n",
    "        resourceArn = cluster_arn,\n",
    "        secretArn = secret_arn_rls,\n",
    "        transactionId = tr['transactionId'])\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Invoke Anthrophic Claude LLM on Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_message(bedrock_runtime, model_id, system_prompt, messages, max_tokens):\n",
    "\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"system\": system_prompt,\n",
    "            \"messages\": messages\n",
    "        }  \n",
    "    )  \n",
    "\n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "   \n",
    "    return response_body\n",
    "\n",
    "def invoke_llm_with_rag(messages):\n",
    "    model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    \n",
    "    response = generate_message (bedrock_runtime, model_id, \"\", messages, 300)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate vector embeddings and insert into vector db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_tenant_document(file_name, tenantid):\n",
    "    # Load the document\n",
    "    loader = PyPDFLoader(file_name)\n",
    "    doc = loader.load()\n",
    "\n",
    "    # split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=10000,\n",
    "    chunk_overlap=150\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(doc)\n",
    "\n",
    "    # generate vector embeddings and insert into vector db\n",
    "    for chunk in chunks:\n",
    "        embedding = generate_vector_embeddings(chunk.page_content)\n",
    "        insert_response = insert_into_vector_db(embedding, chunk.page_content, file_name, tenantid)\n",
    "\n",
    "    return \"Embeddings inserted successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Generate the vector embeddings for Tenant1 and insert into Vector database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document\n",
    "file_name = \"../multi_tenant_survey_reports/Home_Survey_Tenant1.pdf\"\n",
    "loader = PyPDFLoader(file_name)\n",
    "doc = loader.load()\n",
    "\n",
    "# split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "   chunk_size=10000,\n",
    "   chunk_overlap=150\n",
    ")\n",
    "chunks = text_splitter.split_documents(doc)\n",
    "\n",
    "# generate vector embeddings and insert into vector db\n",
    "for chunk in chunks:\n",
    "   embedding = generate_vector_embeddings(chunk.page_content)\n",
    "   insert_response = insert_into_vector_db(embedding, chunk.page_content, file_name, \"Tenant1\")\n",
    "\n",
    "print(\"Embeddings inserted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Review the vector embeddings stored in the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rdsData.execute_statement( resourceArn=cluster_arn, secretArn=secret_arn, database=db_name,\n",
    "        sql=\"SELECT id,metadata,chunks FROM self_managed.kb LIMIT 5; \",\n",
    "    )\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run a user query using the vector embedding. \n",
    "Review the results from the query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query data and convert it to vector embeddings to query from the vector database\n",
    "question = \"What is the condition of the roof in my survey report?\"\n",
    "embedding = generate_vector_embeddings(question)\n",
    "query_response = query_vector_database(embedding)\n",
    "print(query_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Augment the prompt with the context data from the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(retrievalResults):\n",
    "    contexts = []\n",
    "    for retrievedResult in retrievalResults: \n",
    "        for chunk in retrievedResult:\n",
    "            contexts.append(chunk['stringValue'])\n",
    "    return contexts\n",
    "\n",
    "contexts = get_contexts(query_response['records'])\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{contexts}\n",
    "</context\n",
    "Question: {question}\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Invoke the LLM with the augmented prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[{ \"role\":'user', \"content\":[{'type':'text','text': prompt.format(contexts, question)}]}]\n",
    "llm_response = invoke_llm_with_rag(messages)\n",
    "print(llm_response['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Add more tenants and their documents\n",
    "For each tenant generate the vector embeddings of the document and insert the embeddings into the vector database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_response = insert_tenant_document(\"../multi_tenant_survey_reports/Home_Survey_Tenant2.pdf\", \"Tenant2\")\n",
    "print(insert_response);\n",
    "\n",
    "insert_response = insert_tenant_document(\"../multi_tenant_survey_reports/Home_Survey_Tenant3.pdf\", \"Tenant3\")\n",
    "print(insert_response);\n",
    "\n",
    "insert_response = insert_tenant_document(\"../multi_tenant_survey_reports/Home_Survey_Tenant4.pdf\", \"Tenant4\")\n",
    "print(insert_response);\n",
    "\n",
    "insert_response = insert_tenant_document(\"../multi_tenant_survey_reports/Home_Survey_Tenant5.pdf\", \"Tenant5\")\n",
    "print(insert_response);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Run the same user Query against the vector database\n",
    "Review the query response data and particularly the Tenant ID. You will observe that the query had fetched related data chunks from many tenants. Now the questions is how to implement tenant isolation such that a tenant specific question retrieves data only from that tenants document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query data and convert it to vector embeddings to query from the vector database\n",
    "question = \"What is the condition of the roof in my survey report?\"\n",
    "embedding = generate_vector_embeddings(question)\n",
    "query_response = query_vector_database(embedding)\n",
    "print(query_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 : Run the same user query now using RLS\n",
    "Review the query_vector_database_using_rls() function that uses RDS Data API and implements row level security to restrict the query to fetch only tenant specific data from the vector db. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query data and convert it to vector embeddings to query from the vector database\n",
    "question = \"What is the condition of the roof ?\"\n",
    "embedding = generate_vector_embeddings(question)\n",
    "#print(embedding)\n",
    "query_response = query_vector_database_using_rls(embedding, \"Tenant3\")\n",
    "print(query_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 - Augment the retrieved data into the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = get_contexts(query_response['records'])\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{contexts}\n",
    "</context\n",
    "Question: {question}\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Invoke the LLM with the augmented prompt\n",
    "Now the response from the LLM will be based on the context data of the specific tenant. This helps implementing tenant isolation when retrieving data from the vector database for the generative AI use cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[{ \"role\":'user', \"content\":[{'type':'text','text': prompt.format(contexts, question)}]}]\n",
    "llm_response = invoke_llm_with_rag(messages)\n",
    "print(llm_response['content'][0]['text'])"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
