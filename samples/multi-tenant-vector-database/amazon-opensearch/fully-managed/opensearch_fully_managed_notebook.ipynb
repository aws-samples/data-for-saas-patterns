{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully-managed vector store using Amazon OpenSearch serverless collection\n",
    "\n",
    "Prerequisites before you run the cells in this notebook : \n",
    "1. Deploy an Amazon OpenSearch Serverless collection. (If you are at an AWS event, these are pre-provisioned in your account)\n",
    "2. Note the cluster ARN from the Amazon OpenSearch serverless collection. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the boto3 library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U boto3==1.34.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and clients bedrock_agent, bedrock-agent-runtime, bedrock-runtime,S3, iam, aoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "region_name = \"us-west-2\"\n",
    "\n",
    "bedrock_agent_client = boto3.client(\n",
    "    service_name=\"bedrock-agent\", region_name=region_name\n",
    ")\n",
    "bedrock_agent_runtime = boto3.client(\n",
    "    service_name=\"bedrock-agent-runtime\", region_name=region_name\n",
    ")\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=region_name)\n",
    "\n",
    "s3_client = boto3.client(service_name=\"s3\", region_name=region_name)\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "aoss_client = boto3.client('opensearchserverless', region_name=region_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update ARNs & variables from Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the ARN for OpenSearch Serverless Collection\n",
    "collections = aoss_client.list_collections()\n",
    "opensearch_collection_arn = collections['collectionSummaries'][0]['arn']\n",
    "collection_name = collections['collectionSummaries'][0]['name']\n",
    "collection_dns = aoss_client.batch_get_collection(\n",
    "    names=[collection_name]\n",
    ")['collectionDetails'][0]['collectionEndpoint']\n",
    "index_name = \"fully-managed-vector-store-index\"\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "bucket_name = f\"os-multi-tenant-home-survey-reports-{account_id}\"\n",
    "embedding_model_id = \"amazon.titan-embed-text-v1\"\n",
    "\n",
    "\n",
    "print(collection_dns)\n",
    "print(index_name)\n",
    "print(bucket_name)\n",
    "print(embedding_model_id)\n",
    "print(account_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to upload document to S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Function to upload a document to S3\n",
    "def upload_file_to_s3(file_name, bucket, object_name=None):\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "    try:\n",
    "        s3_client.upload_file(file_name, bucket, object_name)\n",
    "        print(f\"File '{file_name}' uploaded to '{bucket}/{object_name}' successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file '{file_name}' to '{bucket}/{object_name}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to create the Bedrock Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the Bedrock Knowledge Base\n",
    "def create_knowledge_base(\n",
    "    name, description, roleArn, embeddingModelArn, opensearchServerlessConfiguration\n",
    "):\n",
    "    create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        roleArn=roleArn,\n",
    "        knowledgeBaseConfiguration={\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embeddingModelArn\n",
    "            },\n",
    "        },\n",
    "        storageConfiguration={\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\": opensearchServerlessConfiguration,\n",
    "        },\n",
    "    )\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Define function to create the datasource in Bedrock Knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the Datasource in Bedrock Knowledge Base\n",
    "def create_datasource_in_knowledge_base(\n",
    "    name, description, knowledgeBaseId, s3Configuration\n",
    "):\n",
    "    create_datasource_response = bedrock_agent_client.create_data_source(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        knowledgeBaseId=knowledgeBaseId,\n",
    "        dataSourceConfiguration={\"type\": \"S3\", \"s3Configuration\": s3Configuration},\n",
    "    )\n",
    "    return create_datasource_response[\"dataSource\"]\n",
    "\n",
    "\n",
    "def wait_for_ingestion(start_job_response):\n",
    "    job = start_job_response[\"ingestionJob\"]\n",
    "    while job[\"status\"] != \"COMPLETE\":\n",
    "        get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            dataSourceId=ds_id,\n",
    "            ingestionJobId=job[\"ingestionJobId\"],\n",
    "        )\n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "    print(job)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to Invoke Anthrophic Claude LLM on Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to invoke the LLM\n",
    "def generate_message(bedrock_runtime, model_id, system_prompt, messages, max_tokens):\n",
    "\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"system\": system_prompt,\n",
    "            \"messages\": messages\n",
    "        }  \n",
    "    )  \n",
    "\n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "   \n",
    "    return response_body\n",
    "\n",
    "def invoke_llm_with_rag(messages):\n",
    "    model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    \n",
    "    response = generate_message (bedrock_runtime, model_id, \"\", messages, 300)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to retrieve vector data chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve chunks from vector store through KB\n",
    "def retrieve(query, kbId, numberOfResults=5):\n",
    "    response = bedrock_agent_runtime.retrieve(\n",
    "        retrievalQuery={\"text\": query},\n",
    "        knowledgeBaseId=kbId,\n",
    "        retrievalConfiguration={\n",
    "            \"vectorSearchConfiguration\": {\"numberOfResults\": numberOfResults}\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to retrieve vector data chunks with filter enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve chunks from vector store through KB\n",
    "def retrieve_with_filters(query, kbId, tenantId, numberOfResults=5):\n",
    "    tenant_filter = {\"equals\": {\"key\": \"tenantid\", \"value\": tenantId}}\n",
    "    response = bedrock_agent_runtime.retrieve(\n",
    "        retrievalQuery={\"text\": query},\n",
    "        knowledgeBaseId=kbId,\n",
    "        retrievalConfiguration={\n",
    "            \"vectorSearchConfiguration\": {\n",
    "                \"numberOfResults\": numberOfResults,\n",
    "                \"filter\": tenant_filter,\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the IAM role and necessary policies for the Bedrock Knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inject these variable values into the policy templates and generate policies using sed. \n",
    "%env region_name = $region_name\n",
    "%env account_id = $account_id\n",
    "%env bucket_name = $bucket_name\n",
    "%env opensearch_collection_arn = $opensearch_collection_arn\n",
    "%env embedding_model_id = $embedding_model_id\n",
    "\n",
    "!sed -e \"s/\\#account_id\\#/$account_id/\" -e \"s/\\#bucket_name\\#/$bucket_name/\" policy-templates/bedrock_data_source_permissions_policy.json > bedrock_data_source_permissions_policy.json\n",
    "!sed -e \"s/\\#embedding_model_id\\#/$embedding_model_id/\" -e \"s/\\#region_name\\#/$region_name/\" policy-templates/bedrock_model_permissions_policy.json > bedrock_model_permissions_policy.json\n",
    "!sed -e \"s/#opensearch_collection_arn#/${opensearch_collection_arn//\\//\\\\/}/g\" policy-templates/bedrock_opensearch_collection_permissions_policy.json > bedrock_opensearch_collection_permissions_policy.json\n",
    "!sed -e \"s/\\#account_id\\#/$account_id/\" -e \"s/\\#region_name\\#/$region_name/\" policy-templates/bedrock_trust_relationship_policy.json > bedrock_trust_relationship_policy.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the role and attach policies\n",
    "\n",
    "# bedrock-kb-service-role\n",
    "!aws iam create-role \\\n",
    "    --role-name bedrock_kb_service_role_os \\\n",
    "    --assume-role-policy-document file://bedrock_trust_relationship_policy.json\n",
    "\n",
    "# bedrock_model_permissions_policy\n",
    "!aws iam create-policy \\\n",
    "    --policy-name bedrock_model_permissions_policy_os \\\n",
    "    --policy-document file://bedrock_model_permissions_policy.json\n",
    "\n",
    "!aws iam attach-role-policy \\\n",
    "    --role-name bedrock_kb_service_role_os \\\n",
    "    --policy-arn \"arn:aws:iam::$account_id:policy/bedrock_model_permissions_policy_os\"\n",
    "\n",
    "# bedrock_opensearch_collection_permissions_policy\n",
    "!aws iam create-policy \\\n",
    "    --policy-name bedrock_opensearch_collection_permissions_policy_os \\\n",
    "    --policy-document file://bedrock_opensearch_collection_permissions_policy.json\n",
    "\n",
    "!aws iam attach-role-policy \\\n",
    "    --role-name bedrock_kb_service_role_os \\\n",
    "    --policy-arn \"arn:aws:iam::$account_id:policy/bedrock_opensearch_collection_permissions_policy_os\"\n",
    "\n",
    "# bedrock_data_source_permissions_policy\n",
    "!aws iam create-policy \\\n",
    "    --policy-name bedrock_data_source_permissions_policy_os \\\n",
    "    --policy-document file://bedrock_data_source_permissions_policy.json\n",
    "\n",
    "!aws iam attach-role-policy \\\n",
    "    --role-name bedrock_kb_service_role_os \\\n",
    "    --policy-arn \"arn:aws:iam::$account_id:policy/bedrock_data_source_permissions_policy_os\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the generated policy json files.\n",
    "!rm *.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Create the Bedrock Knowledge base\n",
    "The first step is to create the Bedrock Knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Create the Bedrock Knowledge Base\n",
    "\n",
    "opensearchServerlessConfiguration = {\n",
    "    \"collectionArn\": str(opensearch_collection_arn),\n",
    "    \"fieldMapping\": {\n",
    "        \"metadataField\": \"metadata\",\n",
    "        \"textField\": \"chunk\",\n",
    "        \"vectorField\": \"embeddings\",\n",
    "    },\n",
    "    \"vectorIndexName\": str(index_name),\n",
    "}\n",
    "chunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\"maxTokens\": 512, \"overlapPercentage\": 20},\n",
    "}\n",
    "\n",
    "embeddingModelArn = (\n",
    "    f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\"\n",
    ")\n",
    "name = f\"home-survey-reports-knowledge-base-os\"\n",
    "description = \"Home Survey Reports - multi tenant knowledge base.\"\n",
    "roleArn = f\"arn:aws:iam::{account_id}:role/bedrock_kb_service_role_os\"\n",
    "\n",
    "kb = create_knowledge_base(\n",
    "    name, description, roleArn, embeddingModelArn, opensearchServerlessConfiguration\n",
    ")\n",
    "kb_id = kb[\"knowledgeBaseId\"]\n",
    "\n",
    "time.sleep(20)\n",
    "print(f\"Knowledge Base created with ID: {kb_id}\")\n",
    "\n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId=kb_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Create a datasource in the Knowledge base\n",
    "Next let us create the datasource ( S3 bucket) and add it into the Knowledge base configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 : Create a datasource in the knowledge base\n",
    "s3_client.create_bucket(\n",
    "    Bucket=bucket_name,\n",
    "    CreateBucketConfiguration={\"LocationConstraint\": region_name}\n",
    ")\n",
    "\n",
    "s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{bucket_name}\",\n",
    "    \"inclusionPrefixes\" : [\"multi_tenant_survey_reports_os/\"]\n",
    "}\n",
    "ds = create_datasource_in_knowledge_base(name, description, kb_id, s3Configuration)\n",
    "print(ds)\n",
    "ds_id = ds[\"dataSourceId\"]\n",
    "print(f\"Datasource created with ID: {ds_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Add Tenant1 document into the datasource \n",
    "Let us add a document for the Tenant1 into the datasource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 : Add Tenant1 document into the datasource (S3 bucket)\n",
    "upload_file_to_s3(\n",
    "    \"../multi_tenant_survey_reports/Home_Survey_Tenant1.pdf\", bucket_name, object_name=\"multi_tenant_survey_reports_os/Home_Survey_Tenant1.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Ingest the document from the datasource into the vector store.\n",
    "The newly added document from the datasource needs to be ingested into the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 : Ingest data from the datasource into the\n",
    "\n",
    "import time\n",
    "start_job_response = bedrock_agent_client.start_ingestion_job(\n",
    "    knowledgeBaseId=kb_id, dataSourceId=ds_id\n",
    ")\n",
    "wait_for_ingestion(start_job_response)\n",
    "\n",
    "print(f\"Datasource ingestion completed\")\n",
    "\n",
    "# Wait to ensure data is consistent in the data store\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Retrieve the vector data chunks that are similar to the user question.\n",
    "Now we can retrieve the vector data chunks from the Tenant1 document based on a user question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 : Retrieve\n",
    "question = \"What is the condition of the roof in my survey report ? \"\n",
    "response = retrieve(question, kb_id)\n",
    "print(response)\n",
    "\n",
    "print(f\"Step5 - Retrieval of vector data completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Augment the prompt with the data chunks retrieved from the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Augment the prompt\n",
    "def get_contexts(retrievalResults):\n",
    "    contexts = []\n",
    "    for retrievedResult in retrievalResults: \n",
    "        contexts.append(retrievedResult['content']['text'])\n",
    "    return contexts\n",
    "\n",
    "contexts = get_contexts(response['retrievalResults'])\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{contexts}\n",
    "</context\n",
    "Question: {question}\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 : Generate the response from the LLM\n",
    "Now we can finally generate the response from the LLM using the augmented prompt as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 : Generate the response from the LLM\n",
    "messages=[{ \"role\":'user', \"content\":[{'type':'text','text': prompt.format(contexts, question)}]}]\n",
    "llm_response = invoke_llm_with_rag(messages)\n",
    "print(llm_response['content'][0]['text'])\n",
    "print(f\"Step7 - Generated response from LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Add more tenants and their documents\n",
    "\n",
    "Let us upload documents for a few more tenants : Tenant2, Tenant3, Tenant4, Tenant5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8 : Add Tenant2, Tenant3, Tenant4 documents into the datasource (S3 bucket)\n",
    "upload_file_to_s3(\"../multi_tenant_survey_reports/Home_Survey_Tenant2.pdf\", bucket_name, object_name=\"multi_tenant_survey_reports_os/Home_Survey_Tenant2.pdf\")\n",
    "upload_file_to_s3(\"../multi_tenant_survey_reports/Home_Survey_Tenant3.pdf\", bucket_name, object_name=\"multi_tenant_survey_reports_os/Home_Survey_Tenant3.pdf\")\n",
    "upload_file_to_s3(\"../multi_tenant_survey_reports/Home_Survey_Tenant4.pdf\", bucket_name, object_name=\"multi_tenant_survey_reports_os/Home_Survey_Tenant4.pdf\")\n",
    "upload_file_to_s3(\"../multi_tenant_survey_reports/Home_Survey_Tenant5.pdf\", bucket_name, object_name=\"multi_tenant_survey_reports_os/Home_Survey_Tenant5.pdf\")\n",
    "\n",
    "print(f\"Step8- Uploaded more tenants documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Ingest the new tenant documents into the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9 : Ingest new documents from the datasource into the vector store\n",
    "\n",
    "start_job_response = bedrock_agent_client.start_ingestion_job(\n",
    "    knowledgeBaseId=kb_id, dataSourceId=ds_id\n",
    ")\n",
    "wait_for_ingestion(start_job_response)\n",
    "\n",
    "# Wait to ensure data is consistent in the data store\n",
    "time.sleep(30)\n",
    "print(f\"Step9 - Ingestion of new documents completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10 : Retrieve the vector data related to Tenant3.\n",
    "Now we can attempt to retrieve vector data based on a question from Tenant3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10 : Retrieve the vector data related to the question of Tenant 1\n",
    "question = \"What is the condition of the roof in my survey report for Tenant3? \"\n",
    "response = retrieve(question, kb_id)\n",
    "\n",
    "print(f\"Step10 - Retrieving vector data for Tenant 1 - complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Review the results retrieved. \n",
    "The response will include data chunks from multiple tenant data. So the question is how do we enforce tenant isolation so that when we retrieve data from the vector store, we are able to limit it to a specific tenants data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11 : Review the results and validate the response. You will observe that the response includes chunks from other tenants as well.\n",
    "\n",
    "print(response)\n",
    "\n",
    "for i in response['retrievalResults']:\n",
    "    print(i['location']['s3Location']['uri'])\n",
    "\n",
    "print(f\"Step11 - Review the results and validate the response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Add metadata tagging to each tenant document. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12 : Add metadata tagging to each tenants document\n",
    "upload_file_to_s3(\n",
    "    \"../metadata_tags/Home_Survey_Tenant1.pdf.metadata.json\",\n",
    "    bucket_name,\n",
    "    \"multi_tenant_survey_reports_os/Home_Survey_Tenant1.pdf.metadata.json\",\n",
    ")\n",
    "\n",
    "upload_file_to_s3(\n",
    "    \"../metadata_tags/Home_Survey_Tenant2.pdf.metadata.json\",\n",
    "    bucket_name,\n",
    "    \"multi_tenant_survey_reports_os/Home_Survey_Tenant2.pdf.metadata.json\",\n",
    ")\n",
    "\n",
    "upload_file_to_s3(\n",
    "    \"../metadata_tags/Home_Survey_Tenant3.pdf.metadata.json\",\n",
    "    bucket_name,\n",
    "    \"multi_tenant_survey_reports_os/Home_Survey_Tenant3.pdf.metadata.json\",\n",
    ")\n",
    "\n",
    "upload_file_to_s3(\n",
    "    \"../metadata_tags/Home_Survey_Tenant4.pdf.metadata.json\",\n",
    "    bucket_name,\n",
    "    \"multi_tenant_survey_reports_os/Home_Survey_Tenant4.pdf.metadata.json\",\n",
    ")\n",
    "\n",
    "upload_file_to_s3(\n",
    "    \"../metadata_tags/Home_Survey_Tenant5.pdf.metadata.json\",\n",
    "    bucket_name,\n",
    "    \"multi_tenant_survey_reports_os/Home_Survey_Tenant5.pdf.metadata.json\",\n",
    ")\n",
    "\n",
    "print(f\"Step12 - Metadata tags for each document added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13 : Ingest the metadata tags into the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13 : Ingest tags datasource into the vector store\n",
    "\n",
    "start_job_response = bedrock_agent_client.start_ingestion_job(\n",
    "    knowledgeBaseId=kb_id, dataSourceId=ds_id\n",
    ")\n",
    "wait_for_ingestion(start_job_response)\n",
    "\n",
    "# Wait to ensure data is consistent in the data store\n",
    "time.sleep(30)\n",
    "print(f\"Step13 - Ingestion completed for new metadata documents \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Retrieve the vector data with tenant filtering enabled.\n",
    "Amazon Bedrock Knowledge Base supports filtering using metadata tags. In the previous step we tagged each document with the tenant-id. During retrieval we can pass a filter configuration using the desired tenant-id to enforce the tenant specific data chunks are only retrieved from the underlying vector store of the knowledge base.  \n",
    "\n",
    "Review the response to validate that the data chunks retrieved are from the specific tenants document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14 : Retrieve with filter enabled for Tenant 3\n",
    "question = \"What is the condition of the roof in my survey report  ? \"\n",
    "response = retrieve_with_filters(question, kb_id, \"Tenant2\")\n",
    "\n",
    "print(response)\n",
    "print(\"---------------------------------------------------------\")\n",
    "for i in response['retrievalResults']:\n",
    "    print(i['location']['s3Location']['uri'])\n",
    "    print(i['metadata'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 15 : Augment context into prompt and generate tenant specific response from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: Augment the prompt\n",
    "def get_contexts(retrievalResults):\n",
    "    contexts = []\n",
    "    for retrievedResult in retrievalResults: \n",
    "        contexts.append(retrievedResult['content']['text'])\n",
    "    return contexts\n",
    "\n",
    "contexts = get_contexts(response['retrievalResults'])\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{contexts}\n",
    "</context\n",
    "Question: {question}\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "#  Generate the tenant specific response from the LLM\n",
    "messages=[{ \"role\":'user', \"content\":[{'type':'text','text': prompt.format(contexts, question)}]}]\n",
    "llm_response = invoke_llm_with_rag(messages)\n",
    "print(llm_response['content'][0]['text'])\n",
    "print(f\"Step15 - Generated tenant specific response from LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detach Policy from role\n",
    "\n",
    "!aws iam detach-role-policy --role-name bedrock_kb_service_role_os --policy-arn \"arn:aws:iam::$account_id:policy/bedrock_data_source_permissions_policy_os\"\n",
    "!aws iam detach-role-policy --role-name bedrock_kb_service_role_os --policy-arn \"arn:aws:iam::$account_id:policy/bedrock_opensearch_collection_permissions_policy_os\"\n",
    "!aws iam detach-role-policy --role-name bedrock_kb_service_role_os --policy-arn \"arn:aws:iam::$account_id:policy/bedrock_model_permissions_policy_os\"\n",
    "\n",
    "# Delete policy\n",
    "!aws iam delete-policy --policy-arn \"arn:aws:iam::$account_id:policy/bedrock_data_source_permissions_policy_os\"\n",
    "!aws iam delete-policy --policy-arn \"arn:aws:iam::$account_id:policy/bedrock_opensearch_collection_permissions_policy_os\"\n",
    "!aws iam delete-policy --policy-arn \"arn:aws:iam::$account_id:policy/bedrock_model_permissions_policy_os\"\n",
    "\n",
    "# Delete role\n",
    "!aws iam delete-role --role-name bedrock_kb_service_role_os\n",
    "\n",
    "# Clean up the bucket\n",
    "!aws s3 rm s3://multi-tenant-home-survey-reports-$account_id/multi_tenant_survey_reports_os --recursive\n",
    "\n",
    "# Delete bedrock knowledge base\n",
    "!aws bedrock-agent delete-knowledge-base --knowledge-base-id $kb_id --region $region_name\n",
    "\n",
    "# Empty the layers LambdaCodeBucket\n",
    "!aws s3 rm s3://development-$account_id --recursive\n",
    "\n",
    "print(\"Clean up completed !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "We now successfully implemented a fully-managed vector store using Amazon OpenSearch and Amazon Bedrock. Also we learnt how to configure tenant isolation using the metadata filtering feature of Amazon Bedrock Knowledge Bases. Overall we used the following features : \n",
    "\n",
    "- Amazon Bedrock Knowledge Bases - Metadata filtering : To achieve tenant isolation. \n",
    "- Amazon OpenSearch collection : To index and store vector embeddings\n",
    "- Amazon Bedrock Titan Embeddings model - Generate vector embeddings. \n",
    "- Amazon Bedrock Anthrophic Claude Foundation model - To generate response back to user. "
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
